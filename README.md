<h3>Repository for the code used to analyze tweets from the TweetScraper repo</h3>
<h3>The DATA directory</h3>
<p>This directory contains <i>florida_data</i>, which contains all raw data collected from the TweetScraper. This directory is divided into subdirectories according to whether they are from government, media, utility, or nonprofit source.</p>
<h3>The FLORIDA directory</h3>
<p>The top level of this directory contains all of the code that is actively being used to do machine learning. <span style="text-decoration: underline;">The programs must be run separately for every tweet source (gov, media, utility, nonprofit), since we are running separate analysis on each source.</span></p>
<ul><li><b>crisislex.py</b>: this is a class containing all of the words found in the crisislex. It is imported in <i>lex_dates.py</i></li>
<li><b>doc2vec.py</b>: this program builds the model to use when predicting on categories in predictdoc2vec.py. The model is stored in the doc2vecmodels directory. It is loaded in predictdoc2vec.py. This is one of the supervised training methods used in predicting the categories based on the manual coding. The manually coded tweets are loaded into this program to train the model on, and are located in <i>training_data/supervised_data/&lt;tweetsource&gt;</i><p>USAGE: python doc2vec.py &lt;tweetsource&gt;</p></li>
<li><b>lex_dates.py</b>: this program is for filtering the raw data from the DATA directory and putting into a tab separated and unique delimiter separated format. This program filters data based on whether the tweet contains a word in the crisislex, cleans any dirty text, and only retains tweets made in September. It produces a file in the <i>training_data</i> directory named &lt;tweetsource&gt;_data.txt. which is used in all supervised and unsupervised learning programs, as well as the vizualization programs. <p>USAGE: python lex_dates.py &lt;tweetsource&gt;</p></li>
<li><b>nmf.py</b>: this program is part of the unsupervised learning portion of the project. It will create topics based on the trends it identifies from the data. Currently the number is set to 5.  It produces two files. The first is the topics file, which is stored in <i>results/&lt;tweetsource&gt;_topics.txt</i>. The second is the weights column file, a file containing the weights related to each tweet from the tf-idf matrix. A more thorough explanation about what the weights column is is in <i>represent.py</i>.<p>USAGE: python nmf.py &lt;tweetsource&gt;</p></li>
<li><b>predictdoc2vec.py</b>: this program loads the model created in <i>doc2vec.py</i> and predicts categories on the unseen data contained in the DATA directory. It creates the file <i>results/&lt;tweetsource&gt;_supervised_doc2vec.csv</i>, which is put on the EECS website in my home directory as well as used in the <i>sup_graphs.py</i> program. It contains the tweets and their predicted categories, as well as their dates and permalinks.</b><<p>USAGE: python predictdoc2vec.py &lt;tweetsource&gt;</p></li>
<li><b>randomforest.py</b>: this program is part of the supervised learning methods in this directory, and the one used most often. It will build a tf-idf matrix from the manually coded tweets and theirlabels, fit it to a random forest classifier, and then make predictions on the unseen data. It produces the file <i>results/&lt;typeoffile&gt;_supervised_rf.csv</i>, which is put on the EECS website in my home directory as well as used in the <i>sup_graphs.py</i> program. It contains the tweets and their predicted categories, as well as their dates and permalinks.</b><p>USAGE: python randomforest.py &lt;tweetsource&gt;</p></li>


