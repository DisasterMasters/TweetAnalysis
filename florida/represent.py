'''

this program is for tweet lookup based on the indices from nmf
there is a weight column from the tf-idf matrix that is saved under W_indices_[typeoffile]
the weight column contains an array that is equal to the length of the tweet array from [typeoffile]_data when tab separated
each index in the weight array corresponds to a tweet at the same index in the tweet array
each index of the weight array contains n entries (n = number of topics chosen in NMF)
these entries contain numbers that correspond to the probability that that tweet at that index belongs to that category
in this program, we look through the indices of the weights array, pick the highest probability from that particular index, look it up in the tweet array, and sort it into that category 


For example:       Tweet at index N:				Weights column at index N when 5 categories are selected in NMF:
			
		   [..., thanks for the hurricane, ...]		[..., [...], [0.001, 0.21, 0.982, 0.003, 0.045], [...], ...]


we see that in the weight column, the entry at index 2 has the highest value
therefore, we conclude that this tweet at the same index belongs to category 3 (categories are indexed from 1)

'''

import sys
import re
import csv
import operator
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

typeoffile = sys.argv[1] 

#choose between utility, media, nonprofit, and government
if typeoffile == 'utility':
        outfile = 'training_data/utility_data.txt'
if typeoffile == 'media':
        outfile = 'training_data/media_data.txt'
if typeoffile == 'nonprofit':
        outfile = 'training_data/nonprofit_data.txt'
if typeoffile == 'gov':
        outfile = 'training_data/gov_data.txt'

#open file, tab separate
docs = []
with open(outfile) as file:
        for line in file:
                docs = line.split("\t")

#get the indices of the categories produced by nmf
def getIndices(full_categories, selections):
        indices = []
        s = selections.read().splitlines()
        clean = []
        for line in full_categories:
                match = re.match(r'^Topic', line)
                if not match:
                        line = line.strip()
                        clean.append(line)
        select = []

        for k, v in enumerate(clean):
                if v in s:
			#select is in case we hand pick certain topics that do not cover the full topic list generated by nmf
                        select.append(v)
                        indices.append(k)

        return indices, select 

#get tweets from the indices
def getTweets(indices, weights, title):
	#parse file as a string
        weights = weights.read()
        regex = r"\[(.*?)\]"
        split_list = []
	#grab the indices and their weights, put in an easier to work with format
        weight_list = re.finditer(regex, weights, re.MULTILINE | re.DOTALL)
        for num, w in enumerate(weight_list):
                for group in range(0, len(w.groups())):
                        split_list.append(w.group(1))

        selections_list = []
	
	#remove excessive spaces, put into a list of lists
        for s in split_list:
                s = re.sub('\s+', ' ', s).strip()
                s = s.split(" ")
                inner_list = []
                for i in indices:
                        inner_list.append(float(s[i]))
                selections_list.append(inner_list)

        tweet_dict = {}

	#grab tweets here
        for i in range(0, len(indices)):
		#get the top tweets per index from the weights column
                tweet_index = computeIndices(selections_list, i)
		#gather the tweets and format them for output
                subtweet_list = compileTweets(tweet_index, title, i)
		#link tweet with topic
                topic = computeSimilarity(subtweet_list, title)
		#put tweet into list under topic
                tweet_dict[topic] = [subtweet_list]
        return tweet_dict

#link tweet with topic
def computeSimilarity(subtweet_list, title):

        scores = {}
	#use cosine similarity to link tweets to topic
        for t in title:
		#get the tweet with the highest score in its category and link it to title
                topic = [t, subtweet_list[0][0]]
                tfidf_vectorizer = TfidfVectorizer()
                topic_matrix = tfidf_vectorizer.fit_transform(topic)
		#compute cosine similarity
                cosine = cosine_similarity(topic_matrix[0:1], topic_matrix[1:2])
                scores[float(cosine)] = t
        t = sorted(scores.keys())
	#get highest score
        return scores[t[-1]]

#get highest score from indices
def computeIndices(selected_list, topic_num):
	
	#find highest weight in weight matrix index
        top_score = {}
        for k, v in enumerate(selected_list):
                val = v[topic_num]
                if val == max(v):
                        top_score[k] = val
	#sort for highest score
        sorted_scores = sorted(top_score.items(), key=operator.itemgetter(1), reverse=True)
        return sorted_scores

#compile the tweets for output
def compileTweets(tweet_index, title, i):

        tweet_list = []
        for t in tweet_index:
				index = t[0]
				tweet = docs[index].split('~+&$!sep779++')[0]
				date = docs[index].split('~+&$!sep779++')[1]
				id = docs[index].split('~+&$!sep779++')[2]
				tweet_list.append([tweet, date, id])

        return tweet_list


categories = open("results/" + typeoffile + "_topics.txt", "r") #the categories for a tweet
weights = open("results/W_indices_" + typeoffile + ".txt", "r") #their weights
selections = open("results/" + typeoffile + "_topics.txt", "r") #categories selected (usually all categories are selected, so we keep this and categories the same)

#compute highest indices
indices, title = getIndices(categories, selections)

#open output file
rep = open("results/unsupervised_" + typeoffile + "_tweets.csv", "w")
#compile tweets for output
tweets = getTweets(indices, weights, title)

#open csv file for writing
rep = csv.writer(rep)
rep.writerow(["Topic", "Tweets", "Date", "Number", "Permalink"])
i = 0
for k, v in tweets.iteritems():
	#write topic on its own row
        rep.writerow([k]) 
        i += 1
        for val in v:
		for l in val:
			#write blank, tweet, date, category number, and permalink
			rep.writerow(["", l[0], l[1], str(i), l[2]])

